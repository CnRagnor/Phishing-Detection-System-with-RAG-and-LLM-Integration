{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installing the Required Libraries\n",
        "! pip install transformers faiss-cpu nltk scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9exe6u9Ci1xx",
        "outputId": "02b9d756-f229-46a6-990c-8d1cd214e56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import faiss\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqKSXWSrjhzO",
        "outputId": "9e10a6b4-c757-41f3-8e05-71650e4e85d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collection\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"naserabdullahalam/phishing-email-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLNVfAIPjqY5",
        "outputId": "61e151d7-f3f5-4b6e-a681-e5d4ca44c313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/naserabdullahalam/phishing-email-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "ling = pd.read_csv('/content/Ling.csv')\n",
        "nazario = pd.read_csv('/content/Nazario.csv')\n",
        "nigerian_fraud = pd.read_csv('/content/Nigerian_Fraud.csv')\n",
        "spam_assasin = pd.read_csv('/content/SpamAssasin.csv')\n",
        "phishing_email = pd.read_csv('/content/phishing_email.csv')\n",
        "enron = pd.read_csv('/content/Enron.csv')\n",
        "ceas_08 = pd.read_csv('/content/CEAS_08.csv')\n",
        "\n",
        "# Combine datasets into a single DataFrame\n",
        "data = pd.concat([ling, nazario, nigerian_fraud, spam_assasin, phishing_email, enron, ceas_08], ignore_index=True)\n",
        "\n",
        "# Display the first few rows of the combined dataset\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "hTlN_t8rkLa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80a6f51-36f9-4557-a106-dd1306cabab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             subject  \\\n",
            "0            job posting - apple-iss research center   \n",
            "1                                                NaN   \n",
            "2  query : letter frequencies for text identifica...   \n",
            "3                                               risk   \n",
            "4                           request book information   \n",
            "\n",
            "                                                body  label sender receiver  \\\n",
            "0  content - length : 3386 apple-iss research cen...    0.0    NaN      NaN   \n",
            "1  lang classification grimes , joseph e . and ba...    0.0    NaN      NaN   \n",
            "2  i am posting this inquiry for sergei atamas ( ...    0.0    NaN      NaN   \n",
            "3  a colleague and i are researching the differin...    0.0    NaN      NaN   \n",
            "4  earlier this morning i was on the phone with a...    0.0    NaN      NaN   \n",
            "\n",
            "  date  urls text_combined  \n",
            "0  NaN   NaN           NaN  \n",
            "1  NaN   NaN           NaN  \n",
            "2  NaN   NaN           NaN  \n",
            "3  NaN   NaN           NaN  \n",
            "4  NaN   NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the column names of the combined dataset\n",
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAryDgT6rr-n",
        "outputId": "1c5cd280-9448-4747-cf01-22fa95cb5761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['subject', 'body', 'label', 'sender', 'receiver', 'date', 'urls',\n",
            "       'text_combined'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and Cleaning the Data\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Ensure the missing resource is downloaded\n",
        "\n",
        "# Example preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        return ''\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the email body content\n",
        "data['processed_text'] = data['body'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows of the preprocessed data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOCsYqzLodQv",
        "outputId": "ba55389a-5adc-4541-a54b-5a372b0c90a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             subject  \\\n",
            "0            job posting - apple-iss research center   \n",
            "1                                                NaN   \n",
            "2  query : letter frequencies for text identifica...   \n",
            "3                                               risk   \n",
            "4                           request book information   \n",
            "\n",
            "                                                body  label sender receiver  \\\n",
            "0  content - length : 3386 apple-iss research cen...    0.0    NaN      NaN   \n",
            "1  lang classification grimes , joseph e . and ba...    0.0    NaN      NaN   \n",
            "2  i am posting this inquiry for sergei atamas ( ...    0.0    NaN      NaN   \n",
            "3  a colleague and i are researching the differin...    0.0    NaN      NaN   \n",
            "4  earlier this morning i was on the phone with a...    0.0    NaN      NaN   \n",
            "\n",
            "  date  urls text_combined                                     processed_text  \n",
            "0  NaN   NaN           NaN  content - length : 3386 apple-iss research cen...  \n",
            "1  NaN   NaN           NaN  lang classification grimes , joseph e . and ba...  \n",
            "2  NaN   NaN           NaN  i am posting this inquiry for sergei atamas ( ...  \n",
            "3  NaN   NaN           NaN  a colleague and i are researching the differin...  \n",
            "4  NaN   NaN           NaN  earlier this morning i was on the phone with a...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using BERT tokenizer to convert the email content into tokens\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the email content and truncate sequences longer than 512 tokens\n",
        "data['tokens'] = data['processed_text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True))\n",
        "\n",
        "# Display the first few rows of the tokenized data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GhBjvqPotR5",
        "outputId": "50f62a05-c3ab-4464-e7fb-47a11b1d0f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             subject  \\\n",
            "0            job posting - apple-iss research center   \n",
            "1                                                NaN   \n",
            "2  query : letter frequencies for text identifica...   \n",
            "3                                               risk   \n",
            "4                           request book information   \n",
            "\n",
            "                                                body  label sender receiver  \\\n",
            "0  content - length : 3386 apple-iss research cen...    0.0    NaN      NaN   \n",
            "1  lang classification grimes , joseph e . and ba...    0.0    NaN      NaN   \n",
            "2  i am posting this inquiry for sergei atamas ( ...    0.0    NaN      NaN   \n",
            "3  a colleague and i are researching the differin...    0.0    NaN      NaN   \n",
            "4  earlier this morning i was on the phone with a...    0.0    NaN      NaN   \n",
            "\n",
            "  date  urls text_combined                                     processed_text  \\\n",
            "0  NaN   NaN           NaN  content - length : 3386 apple-iss research cen...   \n",
            "1  NaN   NaN           NaN  lang classification grimes , joseph e . and ba...   \n",
            "2  NaN   NaN           NaN  i am posting this inquiry for sergei atamas ( ...   \n",
            "3  NaN   NaN           NaN  a colleague and i are researching the differin...   \n",
            "4  NaN   NaN           NaN  earlier this morning i was on the phone with a...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [101, 4180, 1011, 3091, 1024, 27908, 2575, 620...  \n",
            "1  [101, 11374, 5579, 24865, 1010, 3312, 1041, 10...  \n",
            "2  [101, 1045, 2572, 14739, 2023, 9934, 2005, 145...  \n",
            "3  [101, 1037, 11729, 1998, 1045, 2024, 20059, 19...  \n",
            "4  [101, 3041, 2023, 2851, 1045, 2001, 2006, 1996...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad and Truncate Sequences\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Pad and truncate sequences\n",
        "data['tokens'] = data['tokens'].apply(lambda x: x[:MAX_LEN] + [0] * (MAX_LEN - len(x)))\n",
        "\n",
        "# Display the first few rows of the tokenized data\n",
        "print(data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuKxdopoo6vN",
        "outputId": "a585afcf-fd94-41ac-baee-20f6e10d96df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             subject  \\\n",
            "0            job posting - apple-iss research center   \n",
            "1                                                NaN   \n",
            "2  query : letter frequencies for text identifica...   \n",
            "3                                               risk   \n",
            "4                           request book information   \n",
            "\n",
            "                                                body  label sender receiver  \\\n",
            "0  content - length : 3386 apple-iss research cen...    0.0    NaN      NaN   \n",
            "1  lang classification grimes , joseph e . and ba...    0.0    NaN      NaN   \n",
            "2  i am posting this inquiry for sergei atamas ( ...    0.0    NaN      NaN   \n",
            "3  a colleague and i are researching the differin...    0.0    NaN      NaN   \n",
            "4  earlier this morning i was on the phone with a...    0.0    NaN      NaN   \n",
            "\n",
            "  date  urls text_combined                                     processed_text  \\\n",
            "0  NaN   NaN           NaN  content - length : 3386 apple-iss research cen...   \n",
            "1  NaN   NaN           NaN  lang classification grimes , joseph e . and ba...   \n",
            "2  NaN   NaN           NaN  i am posting this inquiry for sergei atamas ( ...   \n",
            "3  NaN   NaN           NaN  a colleague and i are researching the differin...   \n",
            "4  NaN   NaN           NaN  earlier this morning i was on the phone with a...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [101, 4180, 1011, 3091, 1024, 27908, 2575, 620...  \n",
            "1  [101, 11374, 5579, 24865, 1010, 3312, 1041, 10...  \n",
            "2  [101, 1045, 2572, 14739, 2023, 9934, 2005, 145...  \n",
            "3  [101, 1037, 11729, 1998, 1045, 2024, 20059, 19...  \n",
            "4  [101, 3041, 2023, 2851, 1045, 2001, 2006, 1996...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Step 1: Load and Combine Datasets\n",
        "ling = pd.read_csv('/content/Ling.csv')\n",
        "nazario = pd.read_csv('/content/Nazario.csv')\n",
        "nigerian_fraud = pd.read_csv('/content/Nigerian_Fraud.csv')\n",
        "spam_assasin = pd.read_csv('/content/SpamAssasin.csv')\n",
        "phishing_email = pd.read_csv('/content/phishing_email.csv')\n",
        "enron = pd.read_csv('/content/Enron.csv')\n",
        "ceas_08 = pd.read_csv('/content/CEAS_08.csv')\n",
        "\n",
        "# Combine datasets into a single DataFrame\n",
        "data = pd.concat([ling, nazario, nigerian_fraud, spam_assasin, phishing_email, enron, ceas_08], ignore_index=True)\n",
        "\n",
        "# Step 2: Preprocess and Clean the Data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, float):\n",
        "        return ''\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the email body content\n",
        "data['processed_text'] = data['body'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Feature Extraction\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize_and_pad(text):\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encoded['input_ids'].squeeze().tolist(), encoded['attention_mask'].squeeze().tolist()\n",
        "\n",
        "# Tokenize the email content and create attention masks\n",
        "data['tokens'], data['attention_mask'] = zip(*data['processed_text'].apply(tokenize_and_pad))\n",
        "\n",
        "# Display the first few rows of the tokenized data with attention masks\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1L5Rhsppga3",
        "outputId": "19282af9-f15c-4002-840b-ae3234edf3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             subject  \\\n",
            "0            job posting - apple-iss research center   \n",
            "1                                                NaN   \n",
            "2  query : letter frequencies for text identifica...   \n",
            "3                                               risk   \n",
            "4                           request book information   \n",
            "\n",
            "                                                body  label sender receiver  \\\n",
            "0  content - length : 3386 apple-iss research cen...    0.0    NaN      NaN   \n",
            "1  lang classification grimes , joseph e . and ba...    0.0    NaN      NaN   \n",
            "2  i am posting this inquiry for sergei atamas ( ...    0.0    NaN      NaN   \n",
            "3  a colleague and i are researching the differin...    0.0    NaN      NaN   \n",
            "4  earlier this morning i was on the phone with a...    0.0    NaN      NaN   \n",
            "\n",
            "  date  urls text_combined                                     processed_text  \\\n",
            "0  NaN   NaN           NaN  content - length : 3386 apple-iss research cen...   \n",
            "1  NaN   NaN           NaN  lang classification grimes , joseph e . and ba...   \n",
            "2  NaN   NaN           NaN  i am posting this inquiry for sergei atamas ( ...   \n",
            "3  NaN   NaN           NaN  a colleague and i are researching the differin...   \n",
            "4  NaN   NaN           NaN  earlier this morning i was on the phone with a...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  [101, 4180, 1011, 3091, 1024, 27908, 2575, 620...   \n",
            "1  [101, 11374, 5579, 24865, 1010, 3312, 1041, 10...   \n",
            "2  [101, 1045, 2572, 14739, 2023, 9934, 2005, 145...   \n",
            "3  [101, 1037, 11729, 1998, 1045, 2024, 20059, 19...   \n",
            "4  [101, 3041, 2023, 2851, 1045, 2001, 2006, 1996...   \n",
            "\n",
            "                                      attention_mask  \n",
            "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
            "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
            "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
            "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_tokens, X_test_tokens, y_train, y_test = train_test_split(\n",
        "    data['tokens'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "X_train_masks, X_test_masks = train_test_split(\n",
        "    data['attention_mask'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Display the shapes of the splits to verify\n",
        "print(f\"X_train_tokens shape: {len(X_train_tokens)}\")\n",
        "print(f\"X_test_tokens shape: {len(X_test_tokens)}\")\n",
        "print(f\"y_train shape: {len(y_train)}\")\n",
        "print(f\"y_test shape: {len(y_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WE9PZAfdBst",
        "outputId": "250fddfb-04cc-470e-9d50-48515ab8221d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_tokens shape: 119499\n",
            "X_test_tokens shape: 29875\n",
            "y_train shape: 119499\n",
            "y_test shape: 29875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Pre-trained Model\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9_tAKaxpYkK",
        "outputId": "369b94ee-463b-466f-b624-20d523ebcc33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Ensure data is converted to tensors\n",
        "train_inputs = torch.tensor(X_train_tokens)\n",
        "train_masks = torch.tensor(X_train_masks)\n",
        "train_labels = torch.tensor(y_train)\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=8)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Convert the labels to one-hot encoded format\n",
        "def one_hot(labels, num_classes):\n",
        "    return F.one_hot(labels.to(torch.long), num_classes)\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = 2\n",
        "\n",
        "# Train the model (simplified example)\n",
        "model.train()\n",
        "for epoch in range(3):  # Let's train for 3 epochs\n",
        "    for batch in train_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Convert labels to one-hot encoded format\n",
        "        b_labels_one_hot = one_hot(b_labels, num_classes).float()\n",
        "\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels_one_hot)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfjLwpZQ2iaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to tensors\n",
        "train_inputs = torch.tensor(X_train_tokens)\n",
        "train_masks = torch.tensor(X_train_masks)\n",
        "train_labels = torch.tensor(y_train)\n",
        "test_inputs = torch.tensor(X_test_tokens)\n",
        "test_masks = torch.tensor(X_test_masks)\n",
        "test_labels = torch.tensor(y_test)\n"
      ],
      "metadata": {
        "id": "jPkBw0IbKpU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_inputs, attention_mask=test_masks)\n",
        "    predictions = torch.argmax(outputs.logits, axis=1)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "precision = precision_score(test_labels, predictions, average='binary')\n",
        "recall = recall_score(test_labels, predictions, average='binary')\n",
        "f1 = f1_score(test_labels, predictions, average='binary')\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "id": "A0UELL_U8Ykq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask flask-ngrok\n"
      ],
      "metadata": {
        "id": "0yf__j5KNfK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "@app.route('/detect', methods=['POST'])\n",
        "def detect():\n",
        "    email_content = request.json['email_content']\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        email_content,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    tokens_tensor = encoded['input_ids']\n",
        "    attention_mask_tensor = encoded['attention_mask']\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, attention_mask=attention_mask_tensor)\n",
        "    prediction = torch.argmax(outputs.logits, axis=1).item()\n",
        "    return jsonify({'prediction': 'phishing' if prediction == 1 else 'legitimate'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n"
      ],
      "metadata": {
        "id": "j0KOAO5iNmcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X POST -H \"Content-Type: application/json\" -d '{\"email_content\": \"Your email content here\"}' http://<ngrok-public-url>/detect\n"
      ],
      "metadata": {
        "id": "pXAJbcd0OJdo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}